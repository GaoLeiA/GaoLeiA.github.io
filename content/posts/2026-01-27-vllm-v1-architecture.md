---
layout: post
title: vLLM V1 æ¶æ„è¯¦è§£
category: ai
---

## ğŸ“‹ æ¦‚è¿°

vLLM V1 æ˜¯ vLLM é¡¹ç›®çš„æ–°ä¸€ä»£æ¨ç†å¼•æ“æ¶æ„ï¼Œç›¸æ¯”æ—§ç‰ˆè¿›è¡Œäº†å…¨é¢çš„æ¨¡å—åŒ–é‡æ„ã€‚æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç» V1 æ¶æ„ä¸­å„æ¨¡å—çš„åŠŸèƒ½å’Œäº¤äº’å…³ç³»ã€‚

---

## ğŸ—ï¸ æ•´ä½“æ¶æ„

vLLM V1 é‡‡ç”¨åˆ†å±‚æ¶æ„è®¾è®¡ï¼Œä»ä¸Šåˆ°ä¸‹åˆ†ä¸ºä»¥ä¸‹å‡ å±‚ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API Layer (API å±‚)                       â”‚
â”‚              OpenAI API Server / HTTP Endpoints             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   Engine Layer (å¼•æ“å±‚)                      â”‚
â”‚     AsyncLLM / LLMEngine / InputProcessor / Detokenizer     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    Core Layer (æ ¸å¿ƒå±‚)                       â”‚
â”‚        EngineCore / Scheduler / KVCacheManager              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  Executor Layer (æ‰§è¡Œå™¨å±‚)                   â”‚
â”‚      UniprocExecutor / MultiprocExecutor / RayExecutor      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   Worker Layer (å·¥ä½œèŠ‚ç‚¹å±‚)                  â”‚
â”‚        GPUWorker / GPUModelRunner / CPUWorker               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚               Core Components (æ ¸å¿ƒç»„ä»¶å±‚)                   â”‚
â”‚   Attention / Sampling / SpecDecode / StructuredOutput     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ æ¨¡å—è¯¦è§£

### 1. ğŸš€ Engine (å¼•æ“æ¨¡å—) - `vllm/v1/engine/`

å¼•æ“æ¨¡å—æ˜¯ vLLM çš„å…¥å£ç‚¹ï¼Œè´Ÿè´£å¤„ç†ç”¨æˆ·è¯·æ±‚çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸã€‚

| æ–‡ä»¶ | æ ¸å¿ƒç±» | åŠŸèƒ½è¯´æ˜ |
|------|--------|----------|
| `async_llm.py` | `AsyncLLM` | å¼‚æ­¥å¼•æ“ï¼Œæ”¯æŒå¹¶å‘è¯·æ±‚å¤„ç†ï¼Œæ˜¯ç”Ÿäº§ç¯å¢ƒçš„ä¸»è¦æ¥å£ |
| `llm_engine.py` | `LLMEngine` | åŒæ­¥å¼•æ“ï¼Œç”¨äºç®€å•åœºæ™¯å’Œæµ‹è¯• |
| `core.py` | `EngineCore` | å¼•æ“æ ¸å¿ƒé€»è¾‘ï¼Œåè°ƒè°ƒåº¦å™¨å’Œæ‰§è¡Œå™¨ |
| `core_client.py` | `EngineCoreClient` | å¼•æ“æ ¸å¿ƒçš„å®¢æˆ·ç«¯å°è£… |
| `input_processor.py` | `InputProcessor` | è¾“å…¥é¢„å¤„ç†ï¼ŒåŒ…æ‹¬ tokenization å’Œå¤šæ¨¡æ€å¤„ç† |
| `output_processor.py` | `OutputProcessor` | è¾“å‡ºåå¤„ç†ï¼Œæ„å»ºæœ€ç»ˆå“åº” |
| `detokenizer.py` | `Detokenizer` | å°† token ID è½¬æ¢å›æ–‡æœ¬ |
| `logprobs.py` | - | Log æ¦‚ç‡è®¡ç®—å·¥å…· |

**æ ¸å¿ƒæµç¨‹ï¼š**
```python
# EngineCore.step() çš„æ ¸å¿ƒå¾ªç¯
def step(self):
    # 1. è°ƒåº¦ - å†³å®šæœ¬æ¬¡è¿­ä»£å¤„ç†å“ªäº›è¯·æ±‚
    scheduler_output = self.scheduler.schedule()
    
    # 2. æ‰§è¡Œ - è¿è¡Œæ¨¡å‹å‰å‘ä¼ æ’­
    model_output = self.executor.execute_model(scheduler_output)
    
    # 3. æ›´æ–° - å¤„ç†è¾“å‡ºå¹¶æ›´æ–°çŠ¶æ€
    outputs = self.scheduler.update_from_output(scheduler_output, model_output)
    
    return outputs
```

---

### 2. ğŸ“Š Core (æ ¸å¿ƒè°ƒåº¦æ¨¡å—) - `vllm/v1/core/`

æ ¸å¿ƒæ¨¡å—è´Ÿè´£è¯·æ±‚è°ƒåº¦å’Œ KV Cache ç®¡ç†ï¼Œæ˜¯ vLLM é«˜æ€§èƒ½çš„å…³é”®ã€‚

#### 2.1 è°ƒåº¦å™¨ (`sched/`)

| æ–‡ä»¶ | æ ¸å¿ƒç±» | åŠŸèƒ½è¯´æ˜ |
|------|--------|----------|
| `interface.py` | `SchedulerInterface` | è°ƒåº¦å™¨æŠ½è±¡æ¥å£ |
| `scheduler.py` | `Scheduler` | ä¸»è°ƒåº¦å™¨å®ç° (99KBï¼Œæ ¸å¿ƒé€»è¾‘) |
| `async_scheduler.py` | `AsyncScheduler` | å¼‚æ­¥è°ƒåº¦å™¨ |
| `output.py` | `SchedulerOutput` | è°ƒåº¦è¾“å‡ºæ•°æ®ç»“æ„ |
| `request_queue.py` | `RequestQueue` | è¯·æ±‚é˜Ÿåˆ—ç®¡ç† |

**è°ƒåº¦å™¨æ ¸å¿ƒåŠŸèƒ½ï¼š**
- ç®¡ç† **waiting queue** (ç­‰å¾…é˜Ÿåˆ—) å’Œ **running queue** (è¿è¡Œé˜Ÿåˆ—)
- å®ç° **Continuous Batching** (è¿ç»­æ‰¹å¤„ç†)
- æ”¯æŒ **Chunked Prefill** (åˆ†å—é¢„å¡«å……)
- ä¸ KV Cache ç®¡ç†å™¨åè°ƒå†…å­˜åˆ†é…

#### 2.2 KV Cache ç®¡ç†

| æ–‡ä»¶ | æ ¸å¿ƒç±» | åŠŸèƒ½è¯´æ˜ |
|------|--------|----------|
| `kv_cache_manager.py` | `KVCacheManager` | KV Cache ç®¡ç†å™¨ä¸»æ¥å£ |
| `single_type_kv_cache_manager.py` | `SingleTypeKVCacheManager` | å•ç±»å‹ KV Cache ç®¡ç† |
| `kv_cache_coordinator.py` | `KVCacheCoordinator` | å¤šç±»å‹ KV Cache åè°ƒ |
| `block_pool.py` | `BlockPool` | å†…å­˜å—æ± ç®¡ç† |
| `kv_cache_utils.py` | - | KV Cache å·¥å…·å‡½æ•° (66KB) |
| `encoder_cache_manager.py` | `EncoderCacheManager` | ç¼–ç å™¨ç¼“å­˜ (å¤šæ¨¡æ€) |

**KV Cache ç®¡ç†ç‰¹æ€§ï¼š**
- **PagedAttention**: åˆ†é¡µå¼ KV Cacheï¼Œé¿å…å†…å­˜ç¢ç‰‡
- **Prefix Caching**: å‰ç¼€ç¼“å­˜ï¼Œå¤ç”¨ç›¸åŒå‰ç¼€çš„ KV
- **Block Pool**: å—æ± ç®¡ç†ï¼Œé«˜æ•ˆçš„å†…å­˜åˆ†é…/é‡Šæ”¾

---

### 3. âš¡ Executor (æ‰§è¡Œå™¨æ¨¡å—) - `vllm/v1/executor/`

æ‰§è¡Œå™¨è´Ÿè´£å°†è®¡ç®—ä»»åŠ¡åˆ†å‘åˆ°ä¸åŒçš„æ‰§è¡Œç¯å¢ƒã€‚

| æ–‡ä»¶ | æ ¸å¿ƒç±» | åŠŸèƒ½è¯´æ˜ |
|------|--------|----------|
| `abstract.py` | `Executor` | æ‰§è¡Œå™¨æŠ½è±¡åŸºç±» |
| `uniproc_executor.py` | `UniprocExecutor` | å•è¿›ç¨‹æ‰§è¡Œå™¨ |
| `multiproc_executor.py` | `MultiprocExecutor` | å¤šè¿›ç¨‹æ‰§è¡Œå™¨ (Tensor Parallel) |
| `ray_executor.py` | `RayExecutor` | Ray åˆ†å¸ƒå¼æ‰§è¡Œå™¨ |
| `ray_utils.py` | - | Ray å·¥å…·å‡½æ•° |

**æ‰§è¡Œæ¨¡å¼ï¼š**
```
å• GPU:     UniprocExecutor â†’ 1 GPUWorker
å¤š GPU TP:  MultiprocExecutor â†’ N GPUWorkers (Tensor Parallel)
åˆ†å¸ƒå¼:     RayExecutor â†’ è·¨èŠ‚ç‚¹ GPUWorkers
```

---

### 4. ğŸ‘· Worker (å·¥ä½œèŠ‚ç‚¹æ¨¡å—) - `vllm/v1/worker/`

å·¥ä½œèŠ‚ç‚¹æ˜¯å®é™…æ‰§è¡Œæ¨¡å‹æ¨ç†çš„ç»„ä»¶ã€‚

| æ–‡ä»¶ | æ ¸å¿ƒç±» | åŠŸèƒ½è¯´æ˜ | ä»£ç è§„æ¨¡ |
|------|--------|----------|----------|
| `gpu_model_runner.py` | `GPUModelRunner` | **GPU æ¨¡å‹è¿è¡Œå™¨ (æ ¸å¿ƒ)** | **6067 è¡Œ** |
| `gpu_worker.py` | `GPUWorker` | GPU å·¥ä½œèŠ‚ç‚¹ | 41KB |
| `gpu_input_batch.py` | `InputBatch` | GPU è¾“å…¥æ‰¹å¤„ç† | 44KB |
| `cpu_model_runner.py` | `CPUModelRunner` | CPU æ¨¡å‹è¿è¡Œå™¨ | 4KB |
| `cpu_worker.py` | `CPUWorker` | CPU å·¥ä½œèŠ‚ç‚¹ | 8KB |
| `xpu_model_runner.py` | `XPUModelRunner` | Intel XPU è¿è¡Œå™¨ | 1KB |
| `xpu_worker.py` | `XPUWorker` | Intel XPU å·¥ä½œèŠ‚ç‚¹ | 7KB |
| `block_table.py` | `BlockTable` | å—è¡¨ç®¡ç† | 14KB |
| `worker_base.py` | `WorkerBase` | å·¥ä½œèŠ‚ç‚¹åŸºç±» | 14KB |

**GPUModelRunner æ ¸å¿ƒæ–¹æ³•ï¼š**
```python
class GPUModelRunner:
    def __init__(self, vllm_config, device):
        # åˆå§‹åŒ–æ¨¡å‹ã€æ³¨æ„åŠ›åç«¯ã€é‡‡æ ·å™¨ç­‰
        
    def load_model(self):
        # åŠ è½½æ¨¡å‹æƒé‡
        
    def execute_model(self, scheduler_output):
        # 1. å‡†å¤‡è¾“å…¥ (tokens, positions, attention metadata)
        # 2. æ¨¡å‹å‰å‘ä¼ æ’­
        # 3. é‡‡æ ·ç”Ÿæˆ token
        # 4. è¿”å› ModelRunnerOutput
```

---

### 5. ğŸ‘ï¸ Attention (æ³¨æ„åŠ›æ¨¡å—) - `vllm/v1/attention/`

æ³¨æ„åŠ›æ¨¡å—å®ç°é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—ã€‚

| æ–‡ä»¶/ç›®å½• | åŠŸèƒ½è¯´æ˜ |
|-----------|----------|
| `backend.py` | æ³¨æ„åŠ›åç«¯æŠ½è±¡åŸºç±» `AttentionBackend` |
| `selector.py` | è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ³¨æ„åŠ›åç«¯ |
| `backends/` | å…·ä½“åç«¯å®ç° |
| `ops/` | æ³¨æ„åŠ›ç®—å­ |

**æ”¯æŒçš„æ³¨æ„åŠ›åç«¯ï¼š**
- **FlashAttention V2/V3**: NVIDIA GPU ä¼˜åŒ–
- **FlashInfer**: é«˜æ€§èƒ½æ¨ç†ä¼˜åŒ–
- **PagedAttention**: vLLM åŸç”Ÿåˆ†é¡µæ³¨æ„åŠ›
- **MLA (Multi-Latent Attention)**: DeepSeek æ¨¡å‹æ”¯æŒ

---

### 6. ğŸ² Sample (é‡‡æ ·æ¨¡å—) - `vllm/v1/sample/`

é‡‡æ ·æ¨¡å—è´Ÿè´£ä»æ¨¡å‹è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ · tokenã€‚

| æ–‡ä»¶/ç›®å½• | æ ¸å¿ƒç±» | åŠŸèƒ½è¯´æ˜ |
|-----------|--------|----------|
| `sampler.py` | `Sampler` | ä¸»é‡‡æ ·å™¨ |
| `rejection_sampler.py` | `RejectionSampler` | æ‹’ç»é‡‡æ · (æ¨æµ‹è§£ç ) |
| `metadata.py` | `SamplingMetadata` | é‡‡æ ·å…ƒæ•°æ® |
| `logits_processor/` | - | Logits å¤„ç†å™¨ |
| `ops/` | - | é‡‡æ ·ç®—å­ |

**é‡‡æ ·æµç¨‹ï¼š**
```python
def sample(logits, sampling_metadata):
    # 1. åº”ç”¨æ¸©åº¦ (temperature)
    # 2. åº”ç”¨æƒ©ç½š (repetition, frequency, presence)
    # 3. åº”ç”¨ logits processor
    # 4. Top-K / Top-P è¿‡æ»¤
    # 5. é‡‡æ ·æˆ– argmax (greedy)
    # 6. è®¡ç®— logprobs (å¦‚éœ€è¦)
```

---

### 7. ğŸš„ Spec Decode (æ¨æµ‹è§£ç æ¨¡å—) - `vllm/v1/spec_decode/`

æ¨æµ‹è§£ç é€šè¿‡å¹¶è¡ŒéªŒè¯åŠ é€Ÿç”Ÿæˆã€‚

| æ–‡ä»¶ | æ ¸å¿ƒç±» | åŠŸèƒ½è¯´æ˜ |
|------|--------|----------|
| `eagle.py` | `EAGLEDrafter` | **EAGLE æ¨æµ‹è§£ç ** (64KBï¼Œä¸»åŠ›å®ç°) |
| `medusa.py` | `MedusaProposer` | Medusa å¤šå¤´æ¨æµ‹ |
| `draft_model.py` | `DraftModel` | è‰ç¨¿æ¨¡å‹åŸºç±» |
| `ngram_proposer.py` | `NgramProposer` | N-gram æè®®å™¨ |
| `suffix_decoding.py` | - | åç¼€è§£ç  |
| `metrics.py` | - | æ¨æµ‹è§£ç æŒ‡æ ‡ |

**æ¨æµ‹è§£ç åŸç†ï¼š**
```
1. è‰ç¨¿æ¨¡å‹å¿«é€Ÿç”Ÿæˆ K ä¸ªå€™é€‰ token
2. ç›®æ ‡æ¨¡å‹ä¸€æ¬¡æ€§éªŒè¯æ‰€æœ‰å€™é€‰
3. æ¥å—æ­£ç¡®çš„ tokenï¼Œæ‹’ç»é”™è¯¯çš„
4. å¹³å‡åŠ é€Ÿ 2-3x
```

---

### 8. ğŸ“ Structured Output (ç»“æ„åŒ–è¾“å‡ºæ¨¡å—) - `vllm/v1/structured_output/`

æ”¯æŒ JSON Schemaã€æ­£åˆ™è¡¨è¾¾å¼ç­‰çº¦æŸè¾“å‡ºã€‚

| æ–‡ä»¶ | åŠŸèƒ½è¯´æ˜ |
|------|----------|
| `backend_xgrammar.py` | XGrammar åç«¯ (æ¨è) |
| `backend_outlines.py` | Outlines åç«¯ |
| `backend_guidance.py` | Guidance åç«¯ |
| `backend_lm_format_enforcer.py` | LM Format Enforcer |
| `backend_types.py` | åç«¯ç±»å‹å®šä¹‰ |

---

### 9. ğŸ’¾ KV Offload (KV Cache å¸è½½æ¨¡å—) - `vllm/v1/kv_offload/`

å°† KV Cache å¸è½½åˆ° CPU/ç£ç›˜ä»¥æ”¯æŒæ›´é•¿ä¸Šä¸‹æ–‡ã€‚

| æ–‡ä»¶ | åŠŸèƒ½è¯´æ˜ |
|------|----------|
| `abstract.py` | å¸è½½ç®¡ç†å™¨æŠ½è±¡åŸºç±» |
| `arc_manager.py` | ARC ç¼“å­˜ç­–ç•¥ |
| `lru_manager.py` | LRU ç¼“å­˜ç­–ç•¥ |
| `cpu.py` | CPU å¸è½½å®ç° |
| `backends/` | å…·ä½“åç«¯ |
| `worker/` | å¸è½½å·¥ä½œèŠ‚ç‚¹ |

---

### 10. ğŸ“ˆ Metrics (æŒ‡æ ‡ç›‘æ§æ¨¡å—) - `vllm/v1/metrics/`

æ€§èƒ½ç›‘æ§å’ŒæŒ‡æ ‡æ”¶é›†ã€‚

| æ–‡ä»¶ | åŠŸèƒ½è¯´æ˜ |
|------|----------|
| `loggers.py` | æ—¥å¿—è®°å½•å™¨ (49KB) |
| `perf.py` | æ€§èƒ½ç›‘æ§ (44KB) |
| `prometheus.py` | Prometheus å¯¼å‡º |
| `stats.py` | ç»Ÿè®¡ä¿¡æ¯ |
| `reader.py` | æŒ‡æ ‡è¯»å– |

---

## ğŸ”„ è¯·æ±‚å¤„ç†æµç¨‹

```
ç”¨æˆ·è¯·æ±‚
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AsyncLLM      â”‚ â† æ¥æ”¶è¯·æ±‚
â”‚  add_request() â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ InputProcessor â”‚ â† Tokenize + å¤šæ¨¡æ€å¤„ç†
â”‚  process()     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EngineCore    â”‚ â† æ ¸å¿ƒè°ƒåº¦å¾ªç¯
â”‚    step()      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Schedulerâ”‚  â”‚  KVCacheManager â”‚
â”‚schedule()â”‚ â”‚   allocate()    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚               â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
      SchedulerOutput
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Executor     â”‚ â† åˆ†å‘æ‰§è¡Œ
â”‚ execute_model()â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GPUWorker     â”‚
â”‚  GPUModelRunnerâ”‚ â† å®é™…æ¨ç†
â”‚ execute_model()â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model  â”‚  â”‚ Sampler â”‚
â”‚Forward â”‚â†’ â”‚ sample()â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
         ModelRunnerOutput
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Detokenizer   â”‚ â† è§£ç è¾“å‡º
â”‚  detokenize()  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
      ç”¨æˆ·å“åº”
```

---

## ğŸ¯ æ€»ç»“

vLLM V1 æ¶æ„çš„æ ¸å¿ƒè®¾è®¡ç†å¿µï¼š

1. **æ¨¡å—åŒ–**: å„ç»„ä»¶è§£è€¦ï¼Œä¾¿äºæ‰©å±•å’Œæµ‹è¯•
2. **é«˜æ€§èƒ½**: PagedAttention + Continuous Batching
3. **å¯æ‰©å±•**: æ”¯æŒå¤šç§ç¡¬ä»¶åç«¯å’Œåˆ†å¸ƒå¼éƒ¨ç½²
4. **çµæ´»æ€§**: æ”¯æŒæ¨æµ‹è§£ç ã€ç»“æ„åŒ–è¾“å‡ºç­‰é«˜çº§ç‰¹æ€§

**å…³é”®ä»£ç é‡ç»Ÿè®¡ï¼š**
- `gpu_model_runner.py`: **6067 è¡Œ** (æ ¸å¿ƒæ¨ç†é€»è¾‘)
- `scheduler.py`: **99KB** (è°ƒåº¦æ ¸å¿ƒ)
- `kv_cache_utils.py`: **66KB** (KV Cache å·¥å…·)
- `eagle.py`: **64KB** (EAGLE æ¨æµ‹è§£ç )

---

## ğŸ“Š æ¶æ„å›¾

æœ¬æ–‡æ¡£é…å¥—ä»¥ä¸‹æ¶æ„å›¾ï¼Œä½äº `docs/images/` ç›®å½•ï¼š

### 1. vLLM V1 æ•´ä½“æ¶æ„å›¾

å±•ç¤º V1 çš„åˆ†å±‚æ¶æ„è®¾è®¡ï¼ŒåŒ…æ‹¬ API å±‚ã€å¼•æ“å±‚ã€æ ¸å¿ƒå±‚ã€æ‰§è¡Œå™¨å±‚ã€å·¥ä½œèŠ‚ç‚¹å±‚å’Œæ ¸å¿ƒç»„ä»¶å±‚ã€‚

![vLLM V1 æ•´ä½“æ¶æ„](/posts-images/vllm_v1_architecture.png)

---

### 2. æ¨¡å—ä¾èµ–ä¸æ•°æ®æµå›¾

å±•ç¤ºå„æ¨¡å—ä¹‹é—´çš„äº¤äº’å…³ç³»ï¼šVllmConfig é…ç½®ä¸­å¿ƒã€è¯·æ±‚å¤„ç†æµæ°´çº¿ã€EngineCore æ ¸å¿ƒå¾ªç¯ç­‰ã€‚

![vLLM V1 æ¨¡å—ä¾èµ–](/posts-images/vllm_v1_modules.png)

---

### 3. æ³¨æ„åŠ›åç«¯æ¶æ„å›¾

è¯¦ç»†å±•ç¤º 20+ ç§æ³¨æ„åŠ›åç«¯å®ç°ï¼šFlashAttentionã€FlashInferã€Tree Attentionã€Mamba ç­‰ã€‚

![vLLM æ³¨æ„åŠ›åç«¯](/posts-images/vllm_attention_backends.png)

---

### 4. KV Cache æ¶æ„å›¾

PagedAttention æ ¸å¿ƒåŸç†ï¼šGPU å†…å­˜å¸ƒå±€ã€é€»è¾‘åˆ°ç‰©ç†å—æ˜ å°„ã€å‰ç¼€ç¼“å­˜ã€KV Offloadã€‚

![vLLM KV Cache](/posts-images/vllm_kv_cache.png)

---

### 5. æ¨æµ‹è§£ç æ¶æ„å›¾

æ¨æµ‹è§£ç åŠ é€ŸæŠ€æœ¯ï¼šEAGLEã€Medusaã€Ngram æè®®å™¨ï¼Œæ‹’ç»é‡‡æ ·éªŒè¯æµç¨‹ã€‚

![vLLM æ¨æµ‹è§£ç ](/posts-images/vllm_spec_decode.png)

---

### 6. è°ƒåº¦å™¨æ¶æ„å›¾

Continuous Batching å’Œè°ƒåº¦ï¼šè¯·æ±‚çŠ¶æ€æœºã€Scheduler æ ¸å¿ƒé€»è¾‘ã€Chunked Prefillã€‚

![vLLM è°ƒåº¦å™¨](/posts-images/vllm_scheduler.png)

---

### 7. å®Œæ•´è¯·æ±‚æµç¨‹å›¾

ç«¯åˆ°ç«¯è¯·æ±‚å¤„ç†ï¼šä» JSON è¯·æ±‚åˆ°å“åº”è¿”å›çš„å®Œæ•´æ•°æ®æµã€‚

![vLLM å®Œæ•´æµç¨‹](/posts-images/vllm_complete_flow.png)

---

## ğŸ”§ æ·±å…¥æŠ€æœ¯ç»†èŠ‚

### Attention Backends è¯¦è§£

vLLM V1 æ”¯æŒ **20+ ç§æ³¨æ„åŠ›åç«¯**ï¼Œæ ¹æ®ç¡¬ä»¶å’Œæ¨¡å‹è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å®ç°ï¼š

| åç«¯ | æ–‡ä»¶å¤§å° | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|
| `FlashAttentionBackend` | 44KB | NVIDIA A100/H100, FA2/FA3 |
| `FlashInferBackend` | 70KB | ä¼˜åŒ–è§£ç ï¼ŒRagged Tensors |
| `TritonAttentionBackend` | 22KB | å¯ç§»æ¤ï¼Œè‡ªå®šä¹‰ Kernel |
| `TreeAttentionBackend` | 16KB | æ¨æµ‹è§£ç éªŒè¯ |
| `FlexAttentionBackend` | 41KB | çµæ´»æ³¨æ„åŠ›æ¨¡å¼ |
| `ROCmAttentionBackend` | 15KB | AMD GPU |
| `CPUAttentionBackend` | 19KB | CPU å›é€€ |
| `MambaAttentionBackend` | 13KB | Mamba æ¶æ„ |

### GPUModelRunner æ ¸å¿ƒæ–¹æ³•

`GPUModelRunner` æ˜¯ vLLM æœ€æ ¸å¿ƒçš„ç±»ï¼ŒåŒ…å« **6067 è¡Œä»£ç **ï¼š

```python
class GPUModelRunner:
    def __init__(self, vllm_config, device):
        # åˆå§‹åŒ–æ¨¡å‹ã€KV Cacheã€é‡‡æ ·å™¨ç­‰ (700+ è¡Œ)
        
    def load_model(self):
        # åŠ è½½æ¨¡å‹æƒé‡ï¼Œæ”¯æŒåˆ†å¸ƒå¼
        
    def execute_model(self, scheduler_output) -> ModelRunnerOutput:
        # æ ¸å¿ƒæ¨ç†é€»è¾‘
        # 1. å‡†å¤‡è¾“å…¥å¼ é‡
        # 2. æ„å»º AttentionMetadata
        # 3. æ¨¡å‹å‰å‘ä¼ æ’­
        # 4. é‡‡æ ·ç”Ÿæˆ token
        # 5. (å¯é€‰) æ¨æµ‹è§£ç 
        # 6. è¿”å›è¾“å‡º
        
    def _prepare_inputs(self, scheduler_output):
        # å‡†å¤‡ input_ids, positions, slot_mapping ç­‰
        
    def _build_attention_metadata(self, ...):
        # æ„å»ºæ³¨æ„åŠ›å…ƒæ•°æ®
```

### æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

| æŠ€æœ¯ | æè¿° | åŠ é€Ÿæ•ˆæœ |
|------|------|----------|
| **PagedAttention** | åˆ†é¡µå¼ KV Cacheï¼Œæ¶ˆé™¤å†…å­˜ç¢ç‰‡ | å†…å­˜æ•ˆç‡ â†‘ 20-30% |
| **Continuous Batching** | è¿ç»­æ‰¹å¤„ç†ï¼ŒåŠ¨æ€è¯·æ±‚ç®¡ç† | ååé‡ â†‘ 2-3x |
| **Prefix Caching** | å‰ç¼€ç¼“å­˜å¤ç”¨ | ç›¸åŒå‰ç¼€è¯·æ±‚ â†‘ 10x+ |
| **Speculative Decoding** | æ¨æµ‹è§£ç å¹¶è¡ŒéªŒè¯ | å»¶è¿Ÿ â†“ 1.5-3x |
| **Chunked Prefill** | åˆ†å—é¢„å¡«å……ï¼Œäº¤é”™å¤„ç† | TTFT â†“ |
| **CUDA Graphs** | å›¾åŒ–æ‰§è¡Œï¼Œå‡å°‘ CPU å¼€é”€ | å°æ‰¹é‡ â†‘ 30-50% |
| **Tensor Parallelism** | å¼ é‡å¹¶è¡Œï¼Œå¤š GPU åˆ‡åˆ† | å¤§æ¨¡å‹æ”¯æŒ |

---

## ğŸ“š å‚è€ƒèµ„æº

- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [PagedAttention è®ºæ–‡](https://arxiv.org/abs/2309.06180)
- [EAGLE æ¨æµ‹è§£ç ](https://arxiv.org/abs/2401.15077)
- [FlashAttention 2](https://arxiv.org/abs/2307.08691)
