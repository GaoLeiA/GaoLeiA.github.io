---
layout: post
title: vLLM å¤šæ¨¡æ€æ¶æ„æ·±åº¦è§£æ
category: ai
---

## ğŸ“‹ æ¦‚è¿°

vLLM æ”¯æŒå¤šç§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆæ¨ç†ã€‚æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç» vLLM ä¸­å¤šæ¨¡æ€å¤„ç†çš„æ¶æ„è®¾è®¡ã€å·¥ä½œæµç¨‹å’Œå®ç°ç»†èŠ‚ã€‚

---

## ğŸŒˆ æ”¯æŒçš„æ¨¡æ€ç±»å‹

| æ¨¡æ€ | è¾“å…¥ç±»å‹ | å¤„ç†æ–¹å¼ |
|------|----------|----------|
| ğŸ–¼ï¸ **å›¾åƒ** | PIL.Image, Tensor, URL, Base64 | Vision Encoder + Projector |
| ğŸ¥ **è§†é¢‘** | å¸§åºåˆ—, URL, æ–‡ä»¶è·¯å¾„ | é€å¸§å¤„ç†æˆ–è§†é¢‘ç¼–ç å™¨ |
| ğŸ”Š **éŸ³é¢‘** | æ³¢å½¢æ•°ç»„, Tensor, URL | Audio Encoder (Whisperç­‰) |
| ğŸ“ **æ–‡æœ¬** | å­—ç¬¦ä¸², Token IDs | Tokenizer + Text Embedding |

---

## ğŸ—ï¸ æ•´ä½“æ¶æ„

![vLLM å¤šæ¨¡æ€æ¶æ„](/posts-images/multimodal_architecture.png)

### æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | æ–‡ä»¶ä½ç½® | åŠŸèƒ½ |
|------|----------|------|
| **MultiModalRegistry** | `vllm/multimodal/registry.py` | æ¨¡æ€æ³¨å†Œå’Œå¤„ç†å™¨åˆ†å‘ |
| **MultiModalProcessor** | `vllm/multimodal/processing/processor.py` | å¤šæ¨¡æ€è¾“å…¥é¢„å¤„ç† (67KB) |
| **EncoderRunner** | `vllm/v1/worker/gpu/mm/encoder_runner.py` | è§†è§‰ç¼–ç å™¨æ‰§è¡Œ |
| **MultiModalInputs** | `vllm/multimodal/inputs.py` | è¾“å…¥æ•°æ®ç»“æ„å®šä¹‰ (32KB) |

---

## ğŸ“Š å¤„ç†æµç¨‹è¯¦è§£

![å¤šæ¨¡æ€å¤„ç†æµç¨‹](/posts-images/multimodal_processing_flow.png)

### Step 1: ç”¨æˆ·è¾“å…¥

```python
from vllm import LLM, SamplingParams
from PIL import Image

# åŠ è½½å›¾åƒ
image = Image.open("cat.jpg")

# åˆ›å»ºè¯·æ±‚
llm = LLM(model="llava-hf/llava-1.5-7b-hf")
prompt = "<image>\nDescribe this image in detail."

# å¤šæ¨¡æ€è¾“å…¥
output = llm.generate(
    {
        "prompt": prompt,
        "multi_modal_data": {"image": image}
    },
    sampling_params=SamplingParams(max_tokens=256)
)
```

### Step 2: MultiModalRegistry æ£€æµ‹

```python
class MultiModalRegistry:
    """å¤šæ¨¡æ€æ³¨å†Œè¡¨"""
    
    def supports_multimodal_inputs(self, model_config):
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒå¤šæ¨¡æ€"""
        model_cls = self._get_model_cls(model_config)
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å®ç° SupportsMultiModal æ¥å£
        return hasattr(model_cls, "embed_multimodal")
    
    def get_processor(self, model_config):
        """è·å–å¯¹åº”æ¨¡å‹çš„å¤„ç†å™¨"""
        processor_factory = self._processor_factories[model_cls]
        return processor_factory.build_processor(ctx)
```

### Step 3: MultiModalProcessor é¢„å¤„ç†

```python
class MultiModalProcessor:
    """å¤šæ¨¡æ€å¤„ç†å™¨"""
    
    def apply(self, prompt: str, mm_data: dict) -> MultiModalInputsV2:
        """å¤„ç†å¤šæ¨¡æ€è¾“å…¥"""
        
        result = {}
        
        # å¤„ç†å›¾åƒ
        if "image" in mm_data:
            images = mm_data["image"]
            
            # 1. ä½¿ç”¨ HuggingFace å¤„ç†å™¨
            processed = self.hf_processor(
                images=images,
                return_tensors="pt"
            )
            
            # pixel_values: [B, C, H, W]
            result["pixel_values"] = processed["pixel_values"]
            result["image_sizes"] = [(img.width, img.height) for img in images]
            
            # 2. ç”Ÿæˆå ä½ç¬¦ token
            # <image> â†’ [IMG][IMG][IMG]...[IMG] (576 tokens)
            num_patches = self.calculate_num_patches(images)
            placeholder_tokens = [self.image_token_id] * num_patches
        
        return MultiModalInputsV2(
            type="multimodal",
            prompt_token_ids=token_ids,
            mm_kwargs=result,
            mm_placeholders=placeholders
        )
```

### Step 4: å ä½ç¬¦æœºåˆ¶

```
åŸå§‹ Prompt: "What is in <image>?"
                        â†“
å±•å¼€å:      "What is in [IMG][IMG][IMG]...[IMG]?"
                         â””â”€â”€â”€â”€â”€â”€ 576 ä¸ª â”€â”€â”€â”€â”€â”€â”˜
                         (å¯¹åº” 24Ã—24 = 576 ä¸ª patch)

PlaceholderRange:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ modality: "image"               â”‚
â”‚ offset: 12        # èµ·å§‹ä½ç½®    â”‚
â”‚ length: 576       # å ä½ç¬¦æ•°é‡  â”‚
â”‚ is_embed: [True, True, ...]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step 5: è°ƒåº¦å™¨å¤„ç†

```python
class Scheduler:
    def schedule(self):
        # ä¸ºå¤šæ¨¡æ€è¯·æ±‚åˆ†é… KV Cache
        # æ³¨æ„ï¼šå›¾åƒ token å ç”¨çš„ KV Cache ä¸æ–‡æœ¬ç›¸åŒ
        
        for req in requests:
            total_tokens = len(req.prompt_token_ids)  # åŒ…å«å›¾åƒå ä½ç¬¦
            blocks_needed = ceil(total_tokens / block_size)
            self.allocate_kv_cache(req, blocks_needed)
        
        # è°ƒåº¦ç¼–ç å™¨æ‰§è¡Œ
        scheduled_encoder_inputs = {}
        for req in new_prefill_requests:
            if req.has_multimodal:
                scheduled_encoder_inputs[req.id] = req.mm_input_ids
        
        return SchedulerOutput(
            scheduled_requests=...,
            scheduled_encoder_inputs=scheduled_encoder_inputs
        )
```

### Step 6: è§†è§‰ç¼–ç å™¨æ‰§è¡Œ

```python
class EncoderRunner:
    """ç¼–ç å™¨æ‰§è¡Œå™¨"""
    
    def execute_mm_encoder(self, model, mm_hashes, mm_kwargs):
        """æ‰§è¡Œå¤šæ¨¡æ€ç¼–ç å™¨"""
        
        encoder_outputs = []
        
        # æŒ‰æ¨¡æ€åˆ†ç»„å¤„ç†
        for modality, num_items, kwargs in group_mm_kwargs_by_modality(mm_kwargs):
            # è°ƒç”¨æ¨¡å‹çš„å¤šæ¨¡æ€ç¼–ç æ–¹æ³•
            outputs = model.embed_multimodal(**kwargs)
            encoder_outputs.extend(outputs)
        
        # ç¼“å­˜ç¼–ç å™¨è¾“å‡º (é¿å…é‡å¤è®¡ç®—)
        for mm_hash, output in zip(mm_hashes, encoder_outputs):
            self.encoder_cache[mm_hash] = output
        
        return encoder_outputs
```

**è§†è§‰ç¼–ç å™¨å¤„ç†è¿‡ç¨‹**ï¼š

```
è¾“å…¥: pixel_values [1, 3, 336, 336]
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Vision Encoder              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Patch Embedding:                    â”‚
â”‚   336 / 14 = 24 patches per side   â”‚
â”‚   24 Ã— 24 = 576 patches total      â”‚
â”‚                                     â”‚
â”‚ Transformer Blocks (24 layers):    â”‚
â”‚   Self-attention + FFN              â”‚
â”‚                                     â”‚
â”‚ Output: [1, 576, 1024]             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Projector                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Linear/MLP projection:             â”‚
â”‚   1024 â†’ 4096 (LLM hidden size)    â”‚
â”‚                                     â”‚
â”‚ Output: [1, 576, 4096]             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
            image_embeddings
```

### Step 7: åµŒå…¥åˆå¹¶

```python
def get_inputs_embeds(self, model, input_ids, mm_embeds, is_mm_embed):
    """åˆå¹¶æ–‡æœ¬å’Œè§†è§‰åµŒå…¥"""
    
    # 1. è·å–æ–‡æœ¬åµŒå…¥
    text_embeds = model.embed_tokens(input_ids)
    # shape: [seq_len, hidden_size]
    
    # 2. æ›¿æ¢å¤šæ¨¡æ€ä½ç½®çš„åµŒå…¥
    # is_mm_embed æ ‡è®°å“ªäº›ä½ç½®æ˜¯å¤šæ¨¡æ€åµŒå…¥
    final_embeds = text_embeds.clone()
    
    mm_idx = 0
    for i, is_mm in enumerate(is_mm_embed):
        if is_mm:
            final_embeds[i] = mm_embeds[mm_idx]
            mm_idx += 1
    
    return final_embeds
```

**åµŒå…¥åˆå¹¶ç¤ºæ„å›¾**ï¼š

```
Token IDs:  [BOS] [Describe] [this] [IMG] [IMG] ... [IMG] [EOS]
              â†“       â†“        â†“      â†“     â†“        â†“      â†“
Text Embed: [E0]    [E1]     [E2]   [--]  [--]     [--]   [En]
                                     â†“     â†“        â†“
Vision Embed:                      [V0]  [V1]     [V575]
                                     â†“     â†“        â†“
Final:      [E0]    [E1]     [E2]  [V0]  [V1] ... [V575] [En]
```

### Step 8: LLM å‰å‘ä¼ æ’­

```python
def forward(self, inputs_embeds, attention_metadata):
    """LLM å‰å‘ä¼ æ’­"""
    
    # inputs_embeds åŒ…å«æ–‡æœ¬ + è§†è§‰åµŒå…¥
    hidden_states = inputs_embeds
    
    # é€šè¿‡æ‰€æœ‰ Transformer å±‚
    for layer in self.layers:
        hidden_states = layer(
            hidden_states,
            attention_metadata=attention_metadata
        )
    
    # ç”Ÿæˆ logits
    logits = self.lm_head(hidden_states[-1:])  # åªå–æœ€åä¸€ä¸ªä½ç½®
    
    return logits
```

---

## ğŸ¨ æ”¯æŒçš„å¤šæ¨¡æ€æ¨¡å‹

![å¤šæ¨¡æ€æ¨¡å‹æ¶æ„](/posts-images/multimodal_models.png)

### æ¶æ„ç±»å‹å¯¹æ¯”

| æ¶æ„ç±»å‹ | æè¿° | ä»£è¡¨æ¨¡å‹ |
|----------|------|----------|
| **Cross-Attention** | è§†è§‰ç‰¹å¾é€šè¿‡äº¤å‰æ³¨æ„åŠ›æ³¨å…¥ LLM | Flamingo, BLIP-2 |
| **Early Fusion** | è§†è§‰åµŒå…¥ä¸æ–‡æœ¬åµŒå…¥æ‹¼æ¥åè¾“å…¥ LLM | LLaVA, InternVL |
| **Native Multimodal** | ç»Ÿä¸€çš„ Transformer å¤„ç†æ‰€æœ‰æ¨¡æ€ | Fuyu, Pixtral |

### æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨

| æ¨¡å‹ | Vision Encoder | LLM | æ”¯æŒæ¨¡æ€ |
|------|----------------|-----|----------|
| **LLaVA-1.5** | CLIP ViT-L/14 | Vicuna-7B/13B | å›¾åƒ |
| **LLaVA-NeXT** | CLIP ViT-L/14 | Llama-3-8B | å›¾åƒ |
| **Qwen2-VL** | Qwen-ViT | Qwen2-7B/72B | å›¾åƒ+è§†é¢‘ |
| **InternVL2** | InternViT-300M/6B | InternLM2-7B/20B | å›¾åƒ+è§†é¢‘ |
| **Phi-3-Vision** | CLIP | Phi-3-mini | å›¾åƒ |
| **Pixtral** | Native | Mistral-12B | å›¾åƒ |
| **Llama-3.2-Vision** | ViT Adapter | Llama-3.2-11B | å›¾åƒ |
| **Qwen2-Audio** | Whisper | Qwen2-7B | éŸ³é¢‘ |
| **MiniCPM-V** | SigLIP | MiniCPM-3B | å›¾åƒ |

---

## ğŸ”§ æ ¸å¿ƒæ•°æ®ç»“æ„

### PlaceholderRange

```python
@dataclass
class PlaceholderRange:
    """å¤šæ¨¡æ€å ä½ç¬¦ä½ç½®ä¿¡æ¯"""
    
    offset: int          # åœ¨ token åºåˆ—ä¸­çš„èµ·å§‹ä½ç½®
    length: int          # å ä½ç¬¦ token æ•°é‡
    is_embed: list[bool] | None  # å“ªäº›ä½ç½®æ˜¯çœŸæ­£çš„åµŒå…¥
    
    def get_num_embeds(self) -> int:
        """è·å–å®é™…åµŒå…¥æ•°é‡"""
        if self.is_embed is None:
            return self.length
        return sum(self.is_embed)
```

### MultiModalKwargs

```python
class MultiModalKwargs(TypedDict, total=False):
    """å¤šæ¨¡æ€å…³é”®å­—å‚æ•°"""
    
    # å›¾åƒç›¸å…³
    pixel_values: torch.Tensor      # [B, C, H, W]
    image_sizes: list[tuple[int, int]]
    image_embeds: torch.Tensor      # é¢„è®¡ç®—çš„åµŒå…¥
    
    # è§†é¢‘ç›¸å…³
    pixel_values_videos: torch.Tensor  # [B, T, C, H, W]
    video_grid_thw: torch.Tensor       # æ—¶ç©ºç½‘æ ¼ä¿¡æ¯
    
    # éŸ³é¢‘ç›¸å…³
    audio_features: torch.Tensor    # [B, T, D]
    audio_embeds: torch.Tensor
```

### MultiModalFeatureSpec

```python
@dataclass
class MultiModalFeatureSpec:
    """V1 å¼•æ“ä½¿ç”¨çš„å¤šæ¨¡æ€ç‰¹å¾è§„æ ¼"""
    
    data: MultiModalKwargsItem | None  # å¤„ç†åçš„æ•°æ®
    modality: str                       # "image", "video", "audio"
    identifier: str                     # å”¯ä¸€æ ‡è¯†ç¬¦
    mm_position: PlaceholderRange       # ä½ç½®ä¿¡æ¯
    mm_hash: str | None                 # ç”¨äºç¼“å­˜çš„å“ˆå¸Œå€¼
```

---

## ğŸ–¼ï¸ å›¾åƒå¤„ç†è¯¦è§£

### åˆ†è¾¨ç‡å¤„ç†ç­–ç•¥

ä¸åŒæ¨¡å‹æ”¯æŒä¸åŒçš„å›¾åƒåˆ†è¾¨ç‡å¤„ç†ç­–ç•¥ï¼š

| ç­–ç•¥ | æè¿° | æ¨¡å‹ç¤ºä¾‹ |
|------|------|----------|
| **Fixed Resolution** | å›ºå®šç¼©æ”¾åˆ°ç»Ÿä¸€å°ºå¯¸ | LLaVA-1.5 (336Ã—336) |
| **Dynamic Resolution** | ä¿æŒå®½é«˜æ¯”ï¼ŒåŠ¨æ€åˆ‡åˆ† | LLaVA-NeXT, Qwen2-VL |
| **Multi-Scale** | ç”Ÿæˆå¤šå°ºåº¦ç‰¹å¾ | InternVL-1.5 |

### åŠ¨æ€åˆ†è¾¨ç‡ç¤ºä¾‹

```python
# Qwen2-VL åŠ¨æ€åˆ†è¾¨ç‡å¤„ç†
def process_dynamic_resolution(image, min_pixels, max_pixels):
    """
    åŠ¨æ€è°ƒæ•´å›¾åƒåˆ†è¾¨ç‡
    
    è¾“å…¥: 1920Ã—1080 å›¾åƒ
    min_pixels: 256 * 256
    max_pixels: 1280 * 28 * 28
    """
    width, height = image.size
    
    # è®¡ç®—ç¼©æ”¾å› å­
    scale = min(
        max_pixels / (width * height),
        1.0
    )
    scale = max(
        scale,
        min_pixels / (width * height)
    )
    
    # è°ƒæ•´å°ºå¯¸ (ä¿æŒ 28 çš„å€æ•°)
    new_width = round(width * scale / 28) * 28
    new_height = round(height * scale / 28) * 28
    
    return image.resize((new_width, new_height))
```

### Patch è®¡ç®—

```python
def calculate_num_patches(image_size, patch_size=14):
    """
    è®¡ç®—å›¾åƒäº§ç”Ÿçš„ patch æ•°é‡
    
    ç¤ºä¾‹:
    - 336Ã—336 å›¾åƒ, patch_size=14
    - patches_per_side = 336 / 14 = 24
    - total_patches = 24 Ã— 24 = 576
    """
    width, height = image_size
    patches_w = width // patch_size
    patches_h = height // patch_size
    return patches_w * patches_h
```

---

## ğŸ¥ è§†é¢‘å¤„ç†

è§†é¢‘è¢«åˆ†è§£ä¸ºå¸§åºåˆ—è¿›è¡Œå¤„ç†ï¼š

```python
def process_video(video_path, num_frames=8):
    """è§†é¢‘å¤„ç†æµç¨‹"""
    
    # 1. æå–å¸§
    frames = extract_frames(video_path, num_frames=num_frames)
    # frames: List[PIL.Image], length = num_frames
    
    # 2. å¤„ç†æ¯ä¸€å¸§ (ä¸å›¾åƒç›¸åŒ)
    pixel_values_list = []
    for frame in frames:
        processed = process_image(frame)
        pixel_values_list.append(processed)
    
    # 3. å †å æˆè§†é¢‘å¼ é‡
    pixel_values = torch.stack(pixel_values_list, dim=1)
    # shape: [B, T, C, H, W] æˆ– [B, T*num_patches, D]
    
    return pixel_values
```

---

## ğŸ”Š éŸ³é¢‘å¤„ç†

```python
def process_audio(audio_path, target_sr=16000):
    """éŸ³é¢‘å¤„ç†æµç¨‹"""
    
    # 1. åŠ è½½éŸ³é¢‘
    waveform, sample_rate = load_audio(audio_path)
    
    # 2. é‡é‡‡æ ·åˆ°ç›®æ ‡é‡‡æ ·ç‡
    if sample_rate != target_sr:
        waveform = resample(waveform, sample_rate, target_sr)
    
    # 3. æå–ç‰¹å¾ (ä½¿ç”¨ Whisper encoder)
    features = whisper_encoder(waveform)
    # shape: [B, T, D]
    
    return features
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. ç¼–ç å™¨ç¼“å­˜

```python
class EncoderRunner:
    def __init__(self):
        # ç¼“å­˜ç¼–ç å™¨è¾“å‡º,é¿å…é‡å¤è®¡ç®—
        self.encoder_cache: dict[str, torch.Tensor] = {}
    
    def execute_mm_encoder(self, model, mm_hashes, mm_kwargs):
        outputs = []
        for mm_hash, kwargs in zip(mm_hashes, mm_kwargs):
            # æ£€æŸ¥ç¼“å­˜
            if mm_hash in self.encoder_cache:
                outputs.append(self.encoder_cache[mm_hash])
            else:
                output = model.embed_multimodal(**kwargs)
                self.encoder_cache[mm_hash] = output
                outputs.append(output)
        return outputs
```

### 2. æ‰¹é‡ç¼–ç 

```python
# å¤šä¸ªè¯·æ±‚çš„å›¾åƒå¯ä»¥æ‰¹é‡ç¼–ç 
def batch_encode_images(images: list[Image], batch_size=8):
    """æ‰¹é‡ç¼–ç å›¾åƒ"""
    all_outputs = []
    
    for i in range(0, len(images), batch_size):
        batch = images[i:i+batch_size]
        pixel_values = preprocess(batch)  # [B, C, H, W]
        outputs = vision_encoder(pixel_values)  # [B, N, D]
        all_outputs.extend(outputs.unbind(0))
    
    return all_outputs
```

### 3. é¢„è®¡ç®—åµŒå…¥

```python
# æ”¯æŒä¼ å…¥é¢„è®¡ç®—çš„åµŒå…¥,è·³è¿‡ç¼–ç å™¨
output = llm.generate({
    "prompt": prompt,
    "multi_modal_data": {
        "image": precomputed_image_embeds  # torch.Tensor
    }
})
```

---

## ğŸ“ ç›¸å…³ä»£ç ä½ç½®

| æ¨¡å— | æ–‡ä»¶è·¯å¾„ | å¤§å° | åŠŸèƒ½ |
|------|----------|------|------|
| **Registry** | `vllm/multimodal/registry.py` | 16KB | æ¨¡æ€æ³¨å†Œè¡¨ |
| **Processor** | `vllm/multimodal/processing/processor.py` | 67KB | å¤šæ¨¡æ€å¤„ç†å™¨ |
| **Inputs** | `vllm/multimodal/inputs.py` | 32KB | è¾“å…¥æ•°æ®ç»“æ„ |
| **EncoderRunner** | `vllm/v1/worker/gpu/mm/encoder_runner.py` | 7KB | ç¼–ç å™¨æ‰§è¡Œ |
| **Image** | `vllm/multimodal/image.py` | 1KB | å›¾åƒå¤„ç† |
| **Video** | `vllm/multimodal/video.py` | 28KB | è§†é¢‘å¤„ç† |
| **Audio** | `vllm/multimodal/audio.py` | 7KB | éŸ³é¢‘å¤„ç† |
| **Cache** | `vllm/multimodal/cache.py` | 23KB | å¤šæ¨¡æ€ç¼“å­˜ |

---

## ğŸ“š ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€å›¾åƒæ¨ç†

```python
from vllm import LLM, SamplingParams
from PIL import Image

llm = LLM(
    model="llava-hf/llava-1.5-7b-hf",
    trust_remote_code=True,
)

image = Image.open("example.jpg")
prompt = "<image>\nWhat is shown in this image?"

output = llm.generate(
    {
        "prompt": prompt,
        "multi_modal_data": {"image": image}
    },
    SamplingParams(max_tokens=256, temperature=0.7)
)
print(output[0].outputs[0].text)
```

### å¤šå›¾åƒæ¨ç†

```python
images = [Image.open(f"img{i}.jpg") for i in range(3)]
prompt = "<image><image><image>\nCompare these three images."

output = llm.generate(
    {
        "prompt": prompt,
        "multi_modal_data": {"image": images}
    },
    SamplingParams(max_tokens=512)
)
```

### è§†é¢‘æ¨ç†

```python
llm = LLM(model="Qwen/Qwen2-VL-7B-Instruct")

video_path = "example.mp4"
prompt = "<video>\nDescribe what happens in this video."

output = llm.generate(
    {
        "prompt": prompt,
        "multi_modal_data": {"video": video_path}
    },
    SamplingParams(max_tokens=256)
)
```

---

## ğŸ¯ æ€»ç»“

vLLM çš„å¤šæ¨¡æ€æ¶æ„å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **ç»Ÿä¸€æ¥å£** | é€šè¿‡ `multi_modal_data` ç»Ÿä¸€å¤„ç†æ‰€æœ‰æ¨¡æ€ |
| **æ¨¡å‹æ— å…³** | è‡ªåŠ¨é€‚é…ä¸åŒ VLM çš„å¤„ç†æ–¹å¼ |
| **é«˜æ•ˆç¼“å­˜** | ç¼–ç å™¨è¾“å‡ºç¼“å­˜é¿å…é‡å¤è®¡ç®— |
| **åŠ¨æ€åˆ†è¾¨ç‡** | æ”¯æŒå„ç§å›¾åƒå°ºå¯¸å’Œè§†é¢‘å¸§æ•° |
| **æ‰¹é‡å¤„ç†** | å¤šæ¨¡æ€è¾“å…¥æ”¯æŒæ‰¹é‡ç¼–ç  |
| **KV Cache å…¼å®¹** | å›¾åƒ token ä¸æ–‡æœ¬ token ç»Ÿä¸€ç®¡ç† |

è¿™ä½¿å¾— vLLM èƒ½å¤Ÿé«˜æ•ˆåœ°æœåŠ¡å„ç§è§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€åº”ç”¨ã€‚
